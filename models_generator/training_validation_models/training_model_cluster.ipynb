{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "# Inicia sesión en wandb\n",
    "wandb.login()\n",
    "\n",
    "# Configuración de logging detallada\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),  # Salida a consola\n",
    "        logging.FileHandler(\"training_log.txt\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"\n",
    "    Determina el dispositivo óptimo:\n",
    "    - GPU NVIDIA si está disponible (CUDA)\n",
    "    - GPU Apple Silicon si está disponible (MPS)\n",
    "    - CPU en otro caso\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logging.info(\"Usando GPU NVIDIA (CUDA)\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logging.info(\"Usando GPU Apple Silicon (MPS)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logging.info(\"Usando CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def get_num_workers(default_max=40):\n",
    "    \"\"\"\n",
    "    Devuelve el número óptimo de workers, limitado al menor entre el número de núcleos de CPU disponibles y default_max.\n",
    "    \"\"\"\n",
    "    cpu_count = os.cpu_count() or default_max\n",
    "    num_workers = min(cpu_count, default_max)\n",
    "    logging.info(f\"Número de workers configurados: {num_workers}\")\n",
    "    return num_workers\n",
    "\n",
    "\n",
    "def load_data(train_dir, valid_dir, batch_size=64, img_size=224):\n",
    "    # Transformaciones de datos\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((img_size, img_size)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Verificar existencia de directorios\n",
    "    if not os.path.exists(train_dir):\n",
    "        raise ValueError(f\"El directorio de entrenamiento no existe: {train_dir}\")\n",
    "    if not os.path.exists(valid_dir):\n",
    "        raise ValueError(f\"El directorio de validación no existe: {valid_dir}\")\n",
    "\n",
    "    # Cargar conjuntos de datos\n",
    "    train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "    valid_dataset = torchvision.datasets.ImageFolder(valid_dir, transform=transform)\n",
    "\n",
    "    # Configurar el número de workers\n",
    "    num_workers = get_num_workers()\n",
    "\n",
    "    # Determinar si usar pin_memory (útil en GPU)\n",
    "    pin_mem = True if torch.cuda.is_available() else False\n",
    "\n",
    "    # Crear dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_mem,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader, len(train_dataset.classes)\n",
    "\n",
    "\n",
    "def evaluate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            # Mover datos al dispositivo óptimo\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calcular accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(valid_loader)\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train(rank, world_size):\n",
    "    try:\n",
    "        logging.info(f\"Comenzando entrenamiento en rank {rank}\")\n",
    "\n",
    "        # Configuración de hiperparámetros\n",
    "        learning_rate = 1e-3  # Ajuste del learning rate\n",
    "        epochs = 5  # Aumento de epochs a 5\n",
    "\n",
    "        # Configuración de los directorios de datos\n",
    "        train_dir = \"../dataset/training\"\n",
    "        valid_dir = \"../dataset/validation\"\n",
    "\n",
    "        # Cargar datos\n",
    "        train_loader, valid_loader, num_classes = load_data(\n",
    "            train_dir,\n",
    "            valid_dir,\n",
    "            batch_size=64,  # Mantener el tamaño del batch\n",
    "            img_size=224,\n",
    "        )\n",
    "\n",
    "        # Configurar W&B con valores numéricos en lugar de strings\n",
    "        wandb.init(\n",
    "            project=\"Understanding-CNNs\",\n",
    "            config={\n",
    "                \"model\": f\"convnext_large_{epochs}_epochs_{learning_rate}_lr\",\n",
    "                \"epochs\": epochs,\n",
    "                \"batch_size\": train_loader.batch_size,\n",
    "            },\n",
    "            name=f\"convnext_large_{epochs}_epochs_{learning_rate}_lr\",\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Número de clases: {num_classes}\")\n",
    "        logging.info(\n",
    "            f\"Longitud del conjunto de entrenamiento: {len(train_loader.dataset)}\"\n",
    "        )\n",
    "\n",
    "        # Determinar el dispositivo óptimo\n",
    "        device = get_device()\n",
    "\n",
    "        # Preparar el modelo: carga convnext_large preentrenado\n",
    "        model = torchvision.models.convnext_large(\n",
    "            weights=torchvision.models.ConvNeXt_Large_Weights.DEFAULT\n",
    "        )\n",
    "        # Reemplazar la capa final para que coincida con el número de clases del dataset\n",
    "        model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
    "\n",
    "        # Si hay más de una GPU NVIDIA, usar DataParallel\n",
    "        if torch.cuda.device_count() > 1 and device.type == \"cuda\":\n",
    "            logging.info(f\"Usando {torch.cuda.device_count()} GPUs con DataParallel\")\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        # Mover el modelo al dispositivo óptimo\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Definir optimizador, criterio y scheduler\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        logging.info(\"Comenzando bucle de entrenamiento\")\n",
    "\n",
    "        # Bucle de entrenamiento\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "                # Mover datos al dispositivo óptimo\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calcular accuracy para el batch\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "                # Imprimir cada 10 batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    accuracy = 100 * correct_predictions / total_predictions\n",
    "                    logging.info(\n",
    "                        f\"Época {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}, Train Accuracy: {accuracy:.2f}%\"\n",
    "                    )\n",
    "\n",
    "            # Evaluar en el conjunto de validación\n",
    "            val_loss, val_accuracy = evaluate(model, valid_loader, criterion, device)\n",
    "            logging.info(\n",
    "                f\"Época {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\"\n",
    "            )\n",
    "\n",
    "            # Log metrics en wandb utilizando valores numéricos\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": loss.item(),\n",
    "                    \"train_acc\": accuracy,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_acc\": val_accuracy,\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Actualizar el scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "        # Guardar el modelo final\n",
    "        os.makedirs(\"./models\", exist_ok=True)\n",
    "        model_path = f\"./models/convnext_large_{epochs}_epochs_{learning_rate}_lr\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        logging.info(f\"Modelo guardado exitosamente en {model_path}\")\n",
    "        wandb.finish()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error durante el entrenamiento: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Usamos un solo dispositivo para entrenamiento (no se está usando entrenamiento distribuido)\n",
    "    world_size = 1\n",
    "    # En este ejemplo se ignora el parámetro rank para cargar el modelo convnext_large\n",
    "    train(rank=0, world_size=world_size)\n",
    "\n",
    "\n",
    "# EJECUCIÓN\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
