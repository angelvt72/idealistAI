diff --git a/models_generator/train_models.py b/models_generator/train_models.py
index aabef15..3c88639 100644
--- a/models_generator/train_models.py
+++ b/models_generator/train_models.py
@@ -16,7 +16,7 @@ logging.basicConfig(
     format="%(asctime)s - %(levelname)s - %(message)s",
     handlers=[
         logging.StreamHandler(sys.stdout),
-        logging.FileHandler("../logs/training_log.txt"),
+        logging.FileHandler("logs/training_log.txt"),
     ],
 )
 
@@ -167,8 +167,8 @@ def train(rank, world_size, model_choice, learning_rate, epochs, batch_size):
                 "epochs": epochs,
                 "batch_size": train_loader.batch_size,
             },
-            name=f"{model_choice}_{epochs}_epochs_{learning_rate}_lr",
-        )
+            name=f"{model_choice}_{epochs}_epochs_{learning_rate}_lr_2",
+        )   
 
         logging.info(f"Number of classes: {num_classes}")
         logging.info(f"Training set size: {len(train_loader.dataset)}")
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index c0d2cb0..a2a3ef7 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250414_210138-emc7bted/logs/debug-internal.log
\ No newline at end of file
+run-20250418_231110-wnk7ypc2/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index e1622a9..3d641bb 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250414_210138-emc7bted/logs/debug.log
\ No newline at end of file
+run-20250418_231110-wnk7ypc2/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 70e5a91..7036b0c 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250414_210138-emc7bted
\ No newline at end of file
+run-20250418_231110-wnk7ypc2
\ No newline at end of file
